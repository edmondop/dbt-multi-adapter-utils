# Example E-commerce Project

This example demonstrates using `dbt-multi-adapter-utils` to make Spark SQL models portable across multiple databases.

## What's Included

This is a **complete dbt project** with:
- âœ… Real dbt models using Spark-specific SQL
- âœ… Seed data (CSV fixtures)
- âœ… Unit tests (dbt v1.8+)
- âœ… Integration tests for Spark and DuckDB
- âœ… Docker Compose setup for local testing

## The Problem

These models were written for **Spark/Databricks** and use Spark-specific functions:

- `COLLECT_LIST()` - collects values into an array (Spark)
- `DATE_TRUNC()` - truncates date (syntax varies)
- `DATE_ADD()` - adds days to date (syntax varies)
- `CURRENT_DATE()` - gets current date (syntax varies)

These functions **don't work** or have **different syntax** in Postgres, Snowflake, and DuckDB!

## The Solution

Use `dbt-multi-adapter-utils` to generate portable macros that work across all adapters.

### Quick Start: Generate Portable Macros

```bash
# 1. Scan to see what functions are detected
dbt-multi-adapter-utils scan

# 2. Generate the portable macros
dbt-multi-adapter-utils generate

# 3. Check the generated macros
cat macros/portable_functions.sql

# 4. (Optional) Rewrite the models
dbt-multi-adapter-utils rewrite --dry-run  # Preview first
dbt-multi-adapter-utils rewrite           # Actually modify files
```

After running `generate`, you'll see `macros/portable_functions.sql` with implementations for all detected functions across Spark, Postgres, and Snowflake.

## Running Integration Tests

This project is used for automated integration testing. The tests are written in Python using `pytest` and `testcontainers`.

### Run Integration Tests

From the project root:

```bash
# Run all integration tests (DuckDB + Spark with testcontainers)
make integration-tests

# Or run only fast tests (DuckDB, no Spark container)
make integration-tests-fast

# Or run directly with pytest
uv run pytest tests/test_integration_dbt.py -v
```

The integration tests:
1. Copy this example project to a temporary directory
2. Run `dbt-multi-adapter-utils` to generate portable macros
3. Run `dbt seed`, `dbt run`, and `dbt test` on DuckDB
4. Run `dbt seed`, `dbt run`, and `dbt test` on Spark (using testcontainers)
5. Run unit tests with `dbt test --select test_type:unit`
6. Clean up automatically

### Manual Testing

You can also manually test the tool locally:

```bash
# From example_project directory
cd example_project

# Run the tool to generate macros
dbt-multi-adapter-utils scan
dbt-multi-adapter-utils generate
dbt-multi-adapter-utils rewrite

# Test with DuckDB (fast, local)
uv run dbt seed --profiles-dir . --target duckdb
uv run dbt run --profiles-dir . --target duckdb
uv run dbt test --profiles-dir . --target duckdb

# Run only unit tests
uv run dbt test --profiles-dir . --target unittest --select test_type:unit
```

## Project Structure

```
example_project/
â”œâ”€â”€ dbt_project.yml           # dbt project config
â”œâ”€â”€ profiles.yml              # Connection profiles (Spark, DuckDB, unittest)
â”œâ”€â”€ .dbt-multi-adapter.yml    # Config for dbt-multi-adapter-utils
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ schema.yml            # Model documentation + unit tests
â”‚   â”œâ”€â”€ user_activity_summary.sql
â”‚   â”œâ”€â”€ order_items_rollup.sql
â”‚   â””â”€â”€ product_tags.sql
â”œâ”€â”€ seeds/
â”‚   â”œâ”€â”€ raw_user_activities.csv
â”‚   â”œâ”€â”€ raw_order_items.csv
â”‚   â””â”€â”€ raw_product_tags.csv
â””â”€â”€ macros/                   # Empty (generated by tool)
```

**Note**: The `macros/portable_functions.sql` file is generated by `dbt-multi-adapter-utils` and is **not** checked into git.

## Unit Tests

The project includes dbt v1.8+ unit tests in `models/schema.yml`:

- `test_order_items_rollup_aggregates_correctly` - Verifies order aggregation
- `test_user_activity_summary_counts_sessions` - Verifies session counting
- `test_product_tags_aggregates_tags` - Verifies tag aggregation

Unit tests run against static input data (no database required) and are perfect for fast feedback during development.

## Use Case: Local Testing with DuckDB

The main use case for this tool is **fast local unit testing**:

1. **Production**: Run on Spark/Snowflake/BigQuery
2. **Local Development**: Run unit tests on DuckDB (instant startup, no cost)
3. **CI/CD**: Run full integration tests on both targets

This is possible because `dbt-multi-adapter-utils` generates portable macros that work identically across all adapters.

## CI/CD Integration

The GitHub Actions workflow (`.github/workflows/ci.yml`) runs the Python integration tests using testcontainers.

Benefits:
- Integration tests work the same locally and in CI
- No manual docker-compose management
- Testcontainers handles Spark lifecycle automatically
- Easy to debug failures locally with same pytest commands

## What You Get

âœ… **One dbt project** that works on Spark, DuckDB, Postgres, AND Snowflake
âœ… **No runtime dependencies** - just pure dbt macros
âœ… **Fast local testing** - DuckDB for unit tests, Spark for integration
âœ… **Explicit control** - you choose when to rewrite
âœ… **Version control friendly** - generated macros are checked in

## Expected Output

After running `generate`, you'll have `macros/portable_functions.sql` with implementations for:

- `portable_array_agg()` â†’ `COLLECT_LIST` (Spark) / `ARRAY_AGG` (Postgres, Snowflake, DuckDB)
- `portable_timestamp_trunc()` â†’ handles syntax differences across adapters
- `portable_ts_or_ds_add()` â†’ `DATE_ADD` with adapter-specific implementations
- Plus implementations for all other detected functions

Now your models work everywhere! ðŸŽ‰
